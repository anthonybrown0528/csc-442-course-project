{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe8tlaHwW3D8Q03qbaJwA5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonybrown0528/csc-442-course-project/blob/main/notebook/hw6/hw6_vectorize_film_descriptions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FLTFyWl1SVbb"
      },
      "outputs": [],
      "source": [
        "# Import pandas to access the dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Import a string vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "ZwW3TjViTgs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = 'https://raw.githubusercontent.com/anthonybrown0528/csc-442-course-project/refs/heads/main/dataset/clean/netflix_film_imdb_data.csv'\n",
        "netflix_film_imdb_scores_df = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "Z_9pGGXuTBLq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Address text encoding errors"
      ],
      "metadata": {
        "id": "RKzN5S_-hfGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some meaningless characters in the film descriptions due to errors when encoding and decoding text from bytes. The most common errors can be identfied and corrected by mapping the meaningless errors to what they are expected to encode.\n",
        "\n",
        "Source: https://www.i18nqa.com/debug/utf8-debug.html"
      ],
      "metadata": {
        "id": "QwXAcu7h664R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_mapping = {\n",
        "    u'â€“': \"—\", # Use prefix to store unicode string. Source: https://docs.python.org/2/tutorial/introduction.html#unicode-strings\n",
        "    u'â€œ': '\"',\n",
        "    u'â€ \t': '\"',\n",
        "    u'ãƒ™ã‚¤ãƒ–ãƒ¬ãƒ¼ãƒ‰ãƒãƒ¼ã‚¹ãƒˆGT(ã‚¬ãƒ': '', # non-latin characters (removed)\n",
        "    u'à¤†à¤µà¤¾à¤°à¤¾ à¤ªà¤¾à¤—à¤² à¤¦à¥€à¤µà¤¾à¤¨à¤¾': '', # non-latin characters (removed)\n",
        "    u'Ã©': 'é',\n",
        "    u'Ã³': 'ó',\n",
        "    u'â€™': \"'\"\n",
        "}\n",
        "\n",
        "def map_encoding(description):\n",
        "  for key in encoding_mapping:\n",
        "    prev_description = description\n",
        "    description = description.replace(key, encoding_mapping[key])\n",
        "\n",
        "  return description\n",
        "\n",
        "netflix_film_imdb_scores_df['description_x'] = netflix_film_imdb_scores_df['description_x'].apply(map_encoding)"
      ],
      "metadata": {
        "id": "IoH9sem5gnpL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Lemmatization"
      ],
      "metadata": {
        "id": "xU_1bB61-PfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet\n",
        "# Source: https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html\n",
        "# Source: https://www.nltk.org/book/ch05.html\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcRtEcjb-Q5A",
        "outputId": "34e2f27c-ce04-435b-e47a-d6fef38bed1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://cs.nyu.edu/~grishman/jet/guide/PennPOS.htm\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "pos_mapping = {\n",
        "    'NOUN': 'n',\n",
        "    'ADV': 'a',\n",
        "    'NUM': 'n',\n",
        "    'PRON': 'n',\n",
        "    'ADJ': 'a',\n",
        "    'VERB': 'v',\n",
        "    'PRT': 'n',\n",
        "    'X': 'n',\n",
        "    'ADP': 'n',\n",
        "    'CONJ': 'n'\n",
        "}\n",
        "\n",
        "pos_ignore = {\n",
        "  'DET',\n",
        "  '.'\n",
        "}\n",
        "\n",
        "def lemmatize_description(description):\n",
        "  tokens = word_tokenize(description)\n",
        "  tokens = nltk.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "  lemma_sequence = []\n",
        "  for token, pos in tokens:\n",
        "    try:\n",
        "      lemma = lemmatizer.lemmatize(token, pos=pos_mapping[pos])\n",
        "    except:\n",
        "      # Handle cases when the part of speech is not recognized\n",
        "      if pos in pos_ignore or pos == token:\n",
        "        # skip tokens that may not contribute to the meaning of the text\n",
        "        continue\n",
        "\n",
        "      # otherwise add the token without transformation\n",
        "      lemma = token\n",
        "    lemma_sequence.append(lemma)\n",
        "  return ' '.join(lemma_sequence)\n",
        "\n",
        "netflix_film_imdb_scores_df['description_x'] = netflix_film_imdb_scores_df['description_x'].apply(lemmatize_description)"
      ],
      "metadata": {
        "id": "3a8XjzHT-aGq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_film_imdb_scores_df.to_csv('netflix_film_imdb_scores_cleaned_descriptions_df.csv', index=False)"
      ],
      "metadata": {
        "id": "uGI1nEfNscl-"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}